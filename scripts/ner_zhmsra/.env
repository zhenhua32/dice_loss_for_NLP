FILE_NAME=reproduce_zhmsra_dice
REPO_PATH=D:\\code\\github\\dice_loss_for_NLP
MODEL_SCALE=base
DATA_DIR=D:\\code\\github\\dice_loss_for_NLP\\data\\zh_msra
BERT_DIR=D:\\code\\pretrain_model_dir\\bert-base-chinese

TRAIN_BATCH_SIZE=10
EVAL_BATCH_SIZE=1
MAX_LENGTH=275

OPTIMIZER=torch.adam
LR_SCHEDULE=polydecay
LR=2e-5

BERT_DROPOUT=0.1
ACC_GRAD=3
MAX_EPOCH=5
GRAD_CLIP=1.0
WEIGHT_DECAY=0.002
WARMUP_PROPORTION=0.02

LOSS_TYPE=dice
W_START=1
W_END=1
W_SPAN=0.2
DICE_SMOOTH=1
DICE_OHEM=0.3
DICE_ALPHA=0.01
FOCAL_GAMMA=2

PRECISION=16
PROGRESS_BAR=1
VAL_CHECK_INTERVAL=0.25

LOSS_SIGN=${LOSS_TYPE}_${DICE_SMOOTH}_${DICE_OHEM}_${DICE_ALPHA}

OUTPUT_BASE_DIR=D:\\code\\github\\dice_loss_for_NLP\\output
OUTPUT_DIR=${OUTPUT_BASE_DIR}/${FILE_NAME}_${MODEL_SCALE}_${TRAIN_BATCH_SIZE}_${MAX_LENGTH}_${LR}_${LR_SCHEDULE}_${BERT_DROPOUT}_${ACC_GRAD}_${MAX_EPOCH}_${GRAD_CLIP}_${WEIGHT_DECAY}_${WARMUP_PROPORTION}_${W_START}_${W_END}_${W_SPAN}_${LOSS_SIGN}

